{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of heart disease using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:\n",
    "The dataset is from the UCI repository (collected at the Cleveland Clinic Foundation) which has data on 303 patients with 14 health-related attributes (from the UCI Machine Learning Repository) and we are using the parameters to predict coronary artery disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Importing necessary libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.1 | packaged by conda-forge | (default, Mar 13 2019, 13:32:59) [MSC v.1900 64 bit (AMD64)]\n",
      "Pandas: 0.25.3\n",
      "Numpy: 1.18.4\n",
      "Sklearn: 0.22.1\n",
      "Matplotlib: 3.2.1\n",
      "Keras: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print ('Pandas: {}'.format(pd.__version__))\n",
    "print ('Numpy: {}'.format(np.__version__))\n",
    "print ('Sklearn: {}'.format(sklearn.__version__))\n",
    "print ('Matplotlib: {}'.format(matplotlib.__version__))\n",
    "print ('Keras: {}'.format(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available through the University of California, Irvine Machine learning repository. Here is the URL:\n",
    "\n",
    "http:////archive.ics.uci.edu/ml/datasets/Heart+Disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the heart disease dataset\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "\n",
    "# the names will be the names of each column in our pandas DataFrame\n",
    "names = ['age',\n",
    "        'sex',\n",
    "        'cp',\n",
    "        'trestbps',\n",
    "        'chol',\n",
    "        'fbs',\n",
    "        'restecg',\n",
    "        'thalach',\n",
    "        'exang',\n",
    "        'oldpeak',\n",
    "        'slope',\n",
    "        'ca',\n",
    "        'thal',\n",
    "        'class']\n",
    "\n",
    "# read the csv\n",
    "cleveland = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (303, 14)\n",
      "age          67\n",
      "sex           1\n",
      "cp            4\n",
      "trestbps    160\n",
      "chol        286\n",
      "fbs           0\n",
      "restecg       2\n",
      "thalach     108\n",
      "exang         1\n",
      "oldpeak     1.5\n",
      "slope         2\n",
      "ca          3.0\n",
      "thal        3.0\n",
      "class         2\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the DataFrame,and we are looking at an example\n",
    "print ('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print (cleveland.loc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on the independent variables of the dataset:\n",
    "cp: chest pain type(Value 1: typical angina Value 2: atypical angina Value 3: non-aginal pain Value 4: asymptomatic.)\n",
    "trestbps: resting blood\n",
    "chol: serum cholesterol levels in mg/dl\n",
    "restecg: resting electrocardiographic results\n",
    "fbs: fasting blood sugar>120 mg/dl\n",
    "\n",
    "Following parameters derived from electrocardiographic testing:\n",
    "thalach:maximum heart rate achieved (during stress tests).\n",
    "exang:exercise induced angina (1 = yes; 0 = no)\n",
    "oldpeak:ST depression induced by exercise relative to rest\n",
    "slope:the slope of the peak exercise ST segment\n",
    "tha1: 3 = normal; 6 = fixed defect; 7 = reversable defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>?</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "280  57.0  1.0  4.0     110.0  335.0  0.0      0.0    143.0    1.0      3.0   \n",
       "281  47.0  1.0  3.0     130.0  253.0  0.0      0.0    179.0    0.0      0.0   \n",
       "282  55.0  0.0  4.0     128.0  205.0  0.0      1.0    130.0    1.0      2.0   \n",
       "283  35.0  1.0  2.0     122.0  192.0  0.0      0.0    174.0    0.0      0.0   \n",
       "284  61.0  1.0  4.0     148.0  203.0  0.0      0.0    161.0    0.0      0.0   \n",
       "285  58.0  1.0  4.0     114.0  318.0  0.0      1.0    140.0    0.0      4.4   \n",
       "286  58.0  0.0  4.0     170.0  225.0  1.0      2.0    146.0    1.0      2.8   \n",
       "287  58.0  1.0  2.0     125.0  220.0  0.0      0.0    144.0    0.0      0.4   \n",
       "288  56.0  1.0  2.0     130.0  221.0  0.0      2.0    163.0    0.0      0.0   \n",
       "289  56.0  1.0  2.0     120.0  240.0  0.0      0.0    169.0    0.0      0.0   \n",
       "290  67.0  1.0  3.0     152.0  212.0  0.0      2.0    150.0    0.0      0.8   \n",
       "291  55.0  0.0  2.0     132.0  342.0  0.0      0.0    166.0    0.0      1.2   \n",
       "292  44.0  1.0  4.0     120.0  169.0  0.0      0.0    144.0    1.0      2.8   \n",
       "293  63.0  1.0  4.0     140.0  187.0  0.0      2.0    144.0    1.0      4.0   \n",
       "294  63.0  0.0  4.0     124.0  197.0  0.0      0.0    136.0    1.0      0.0   \n",
       "295  41.0  1.0  2.0     120.0  157.0  0.0      0.0    182.0    0.0      0.0   \n",
       "296  59.0  1.0  4.0     164.0  176.0  1.0      2.0     90.0    0.0      1.0   \n",
       "297  57.0  0.0  4.0     140.0  241.0  0.0      0.0    123.0    1.0      0.2   \n",
       "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       "302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n",
       "\n",
       "     slope   ca thal  class  \n",
       "280    2.0  1.0  7.0      2  \n",
       "281    1.0  0.0  3.0      0  \n",
       "282    2.0  1.0  7.0      3  \n",
       "283    1.0  0.0  3.0      0  \n",
       "284    1.0  1.0  7.0      2  \n",
       "285    3.0  3.0  6.0      4  \n",
       "286    2.0  2.0  6.0      2  \n",
       "287    2.0    ?  7.0      0  \n",
       "288    1.0  0.0  7.0      0  \n",
       "289    3.0  0.0  3.0      0  \n",
       "290    2.0  0.0  7.0      1  \n",
       "291    1.0  0.0  3.0      0  \n",
       "292    3.0  0.0  6.0      2  \n",
       "293    1.0  2.0  7.0      2  \n",
       "294    2.0  0.0  3.0      1  \n",
       "295    1.0  0.0  3.0      0  \n",
       "296    2.0  2.0  6.0      3  \n",
       "297    2.0  0.0  7.0      1  \n",
       "298    2.0  0.0  7.0      1  \n",
       "299    2.0  2.0  7.0      2  \n",
       "300    2.0  1.0  7.0      3  \n",
       "301    2.0  1.0  3.0      1  \n",
       "302    1.0    ?  3.0      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the last twenty or so data points\n",
    "cleveland.loc[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "280  57.0  1.0  4.0     110.0  335.0  0.0      0.0    143.0    1.0      3.0   \n",
       "281  47.0  1.0  3.0     130.0  253.0  0.0      0.0    179.0    0.0      0.0   \n",
       "282  55.0  0.0  4.0     128.0  205.0  0.0      1.0    130.0    1.0      2.0   \n",
       "283  35.0  1.0  2.0     122.0  192.0  0.0      0.0    174.0    0.0      0.0   \n",
       "284  61.0  1.0  4.0     148.0  203.0  0.0      0.0    161.0    0.0      0.0   \n",
       "285  58.0  1.0  4.0     114.0  318.0  0.0      1.0    140.0    0.0      4.4   \n",
       "286  58.0  0.0  4.0     170.0  225.0  1.0      2.0    146.0    1.0      2.8   \n",
       "287  58.0  1.0  2.0     125.0  220.0  0.0      0.0    144.0    0.0      0.4   \n",
       "288  56.0  1.0  2.0     130.0  221.0  0.0      2.0    163.0    0.0      0.0   \n",
       "289  56.0  1.0  2.0     120.0  240.0  0.0      0.0    169.0    0.0      0.0   \n",
       "290  67.0  1.0  3.0     152.0  212.0  0.0      2.0    150.0    0.0      0.8   \n",
       "291  55.0  0.0  2.0     132.0  342.0  0.0      0.0    166.0    0.0      1.2   \n",
       "292  44.0  1.0  4.0     120.0  169.0  0.0      0.0    144.0    1.0      2.8   \n",
       "293  63.0  1.0  4.0     140.0  187.0  0.0      2.0    144.0    1.0      4.0   \n",
       "294  63.0  0.0  4.0     124.0  197.0  0.0      0.0    136.0    1.0      0.0   \n",
       "295  41.0  1.0  2.0     120.0  157.0  0.0      0.0    182.0    0.0      0.0   \n",
       "296  59.0  1.0  4.0     164.0  176.0  1.0      2.0     90.0    0.0      1.0   \n",
       "297  57.0  0.0  4.0     140.0  241.0  0.0      0.0    123.0    1.0      0.2   \n",
       "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       "302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n",
       "\n",
       "     slope   ca thal  class  \n",
       "280    2.0  1.0  7.0      2  \n",
       "281    1.0  0.0  3.0      0  \n",
       "282    2.0  1.0  7.0      3  \n",
       "283    1.0  0.0  3.0      0  \n",
       "284    1.0  1.0  7.0      2  \n",
       "285    3.0  3.0  6.0      4  \n",
       "286    2.0  2.0  6.0      2  \n",
       "287    2.0  NaN  7.0      0  \n",
       "288    1.0  0.0  7.0      0  \n",
       "289    3.0  0.0  3.0      0  \n",
       "290    2.0  0.0  7.0      1  \n",
       "291    1.0  0.0  3.0      0  \n",
       "292    3.0  0.0  6.0      2  \n",
       "293    1.0  2.0  7.0      2  \n",
       "294    2.0  0.0  3.0      1  \n",
       "295    1.0  0.0  3.0      0  \n",
       "296    2.0  2.0  6.0      3  \n",
       "297    2.0  0.0  7.0      1  \n",
       "298    2.0  0.0  7.0      1  \n",
       "299    2.0  2.0  7.0      2  \n",
       "300    2.0  1.0  7.0      3  \n",
       "301    2.0  1.0  3.0      1  \n",
       "302    1.0  NaN  3.0      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove missing data which is indicated with a \"?\"\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "280  57.0  1.0  4.0     110.0  335.0  0.0      0.0    143.0    1.0      3.0   \n",
       "281  47.0  1.0  3.0     130.0  253.0  0.0      0.0    179.0    0.0      0.0   \n",
       "282  55.0  0.0  4.0     128.0  205.0  0.0      1.0    130.0    1.0      2.0   \n",
       "283  35.0  1.0  2.0     122.0  192.0  0.0      0.0    174.0    0.0      0.0   \n",
       "284  61.0  1.0  4.0     148.0  203.0  0.0      0.0    161.0    0.0      0.0   \n",
       "285  58.0  1.0  4.0     114.0  318.0  0.0      1.0    140.0    0.0      4.4   \n",
       "286  58.0  0.0  4.0     170.0  225.0  1.0      2.0    146.0    1.0      2.8   \n",
       "288  56.0  1.0  2.0     130.0  221.0  0.0      2.0    163.0    0.0      0.0   \n",
       "289  56.0  1.0  2.0     120.0  240.0  0.0      0.0    169.0    0.0      0.0   \n",
       "290  67.0  1.0  3.0     152.0  212.0  0.0      2.0    150.0    0.0      0.8   \n",
       "291  55.0  0.0  2.0     132.0  342.0  0.0      0.0    166.0    0.0      1.2   \n",
       "292  44.0  1.0  4.0     120.0  169.0  0.0      0.0    144.0    1.0      2.8   \n",
       "293  63.0  1.0  4.0     140.0  187.0  0.0      2.0    144.0    1.0      4.0   \n",
       "294  63.0  0.0  4.0     124.0  197.0  0.0      0.0    136.0    1.0      0.0   \n",
       "295  41.0  1.0  2.0     120.0  157.0  0.0      0.0    182.0    0.0      0.0   \n",
       "296  59.0  1.0  4.0     164.0  176.0  1.0      2.0     90.0    0.0      1.0   \n",
       "297  57.0  0.0  4.0     140.0  241.0  0.0      0.0    123.0    1.0      0.2   \n",
       "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       "\n",
       "     slope   ca thal  class  \n",
       "280    2.0  1.0  7.0      2  \n",
       "281    1.0  0.0  3.0      0  \n",
       "282    2.0  1.0  7.0      3  \n",
       "283    1.0  0.0  3.0      0  \n",
       "284    1.0  1.0  7.0      2  \n",
       "285    3.0  3.0  6.0      4  \n",
       "286    2.0  2.0  6.0      2  \n",
       "288    1.0  0.0  7.0      0  \n",
       "289    3.0  0.0  3.0      0  \n",
       "290    2.0  0.0  7.0      1  \n",
       "291    1.0  0.0  3.0      0  \n",
       "292    3.0  0.0  6.0      2  \n",
       "293    1.0  2.0  7.0      2  \n",
       "294    2.0  0.0  3.0      1  \n",
       "295    1.0  0.0  3.0      0  \n",
       "296    2.0  2.0  6.0      3  \n",
       "297    2.0  0.0  7.0      1  \n",
       "298    2.0  0.0  7.0      1  \n",
       "299    2.0  2.0  7.0      2  \n",
       "300    2.0  1.0  7.0      3  \n",
       "301    2.0  1.0  3.0      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with NaN values from DataFrame\n",
    "data = data.dropna(axis=0)\n",
    "data.loc[280:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 14)\n",
      "age         float64\n",
      "sex         float64\n",
      "cp          float64\n",
      "trestbps    float64\n",
      "chol        float64\n",
      "fbs         float64\n",
      "restecg     float64\n",
      "thalach     float64\n",
      "exang       float64\n",
      "oldpeak     float64\n",
      "slope       float64\n",
      "ca           object\n",
      "thal         object\n",
      "class         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the shape and data type of the dataframe\n",
    "print (data.shape)\n",
    "print (data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         float64\n",
       "sex         float64\n",
       "cp          float64\n",
       "trestbps    float64\n",
       "chol        float64\n",
       "fbs         float64\n",
       "restecg     float64\n",
       "thalach     float64\n",
       "exang       float64\n",
       "oldpeak     float64\n",
       "slope       float64\n",
       "ca          float64\n",
       "thal        float64\n",
       "class         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data to numeric to enable further analysis\n",
    "data = data.apply(pd.to_numeric)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.542088</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>3.158249</td>\n",
       "      <td>131.693603</td>\n",
       "      <td>247.350168</td>\n",
       "      <td>0.144781</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>149.599327</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>1.602694</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>4.730640</td>\n",
       "      <td>0.946128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.049736</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.964859</td>\n",
       "      <td>17.762806</td>\n",
       "      <td>51.997583</td>\n",
       "      <td>0.352474</td>\n",
       "      <td>0.994914</td>\n",
       "      <td>22.941562</td>\n",
       "      <td>0.469761</td>\n",
       "      <td>1.166123</td>\n",
       "      <td>0.618187</td>\n",
       "      <td>0.938965</td>\n",
       "      <td>1.938629</td>\n",
       "      <td>1.234551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
       "mean    54.542088    0.676768    3.158249  131.693603  247.350168    0.144781   \n",
       "std      9.049736    0.468500    0.964859   17.762806   51.997583    0.352474   \n",
       "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
       "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
       "50%     56.000000    1.000000    3.000000  130.000000  243.000000    0.000000   \n",
       "75%     61.000000    1.000000    4.000000  140.000000  276.000000    0.000000   \n",
       "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
       "mean     0.996633  149.599327    0.326599    1.055556    1.602694    0.676768   \n",
       "std      0.994914   22.941562    0.469761    1.166123    0.618187    0.938965   \n",
       "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "25%      0.000000  133.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000   \n",
       "75%      2.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
       "\n",
       "             thal       class  \n",
       "count  297.000000  297.000000  \n",
       "mean     4.730640    0.946128  \n",
       "std      1.938629    1.234551  \n",
       "min      3.000000    0.000000  \n",
       "25%      3.000000    0.000000  \n",
       "50%      3.000000    0.000000  \n",
       "75%      7.000000    2.000000  \n",
       "max      7.000000    4.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data characteristics, usings pandas built-in describe() function\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hwahwa\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:307: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "C:\\Users\\Hwahwa\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:307: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  layout[ax.rowNum, ax.colNum] = ax.get_visible()\n",
      "C:\\Users\\Hwahwa\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:313: MatplotlibDeprecationWarning: \n",
      "The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n",
      "C:\\Users\\Hwahwa\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:313: MatplotlibDeprecationWarning: \n",
      "The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead.\n",
      "  if not layout[ax.rowNum + 1, ax.colNum]:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histograms for each variable\n",
    "data.hist(figsize = (12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Create Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed the data appropriately, we can split it into training and testings datasets. We will use Sklearn's train_test_split() function to generate a training dataset (80 percent of the total data) and testing dataset (20 percent of the total data).\n",
    "\n",
    "Furthermore, the class values in this dataset contain multiple types of heart disease with values ranging from 0 (healthy) to 4 (severe heart disease). Consequently, we will need to convert our class data to categorical labels. For example, the label 2 will become [0, 0, 1, 0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and Y datasets for training\n",
    "from sklearn import model_selection\n",
    "\n",
    "X = np.array(data.drop(['class'], 1))\n",
    "y = np.array(data['class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 5)\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# convert the data to categorical labels\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train, num_classes=None)\n",
    "Y_test = to_categorical(y_test, num_classes=None)\n",
    "print (Y_train.shape)\n",
    "print (Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3. Building and Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data fully processed and split into training and testing datasets, we can begin building a neural network to solve this classification problem. Using keras, we will define a simple neural network with one hidden layer. Since this is a categorical classification problem, we will use a softmax activation function in the final layer of our network and a categorical_crossentropy loss during our training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 8)                 112       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 25        \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# define a function to build the keras model\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 237 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C18229DBF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C18229DBF8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "237/237 [==============================] - 0s 2ms/sample - loss: 1.4778 - accuracy: 0.5063\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 1.3881 - accuracy: 0.5612\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.3009 - accuracy: 0.5612\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.2454 - accuracy: 0.5612\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.2220 - accuracy: 0.5612\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.1991 - accuracy: 0.5612\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.1760 - accuracy: 0.5612\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.1596 - accuracy: 0.5612\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.1449 - accuracy: 0.5612\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.1288 - accuracy: 0.5612\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.1114 - accuracy: 0.5612\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0942 - accuracy: 0.5696\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0859 - accuracy: 0.5696\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 1.0921 - accuracy: 0.5738\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0702 - accuracy: 0.5823\n",
      "Epoch 16/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.0612 - accuracy: 0.5696\n",
      "Epoch 17/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0459 - accuracy: 0.5823\n",
      "Epoch 18/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.0342 - accuracy: 0.5823\n",
      "Epoch 19/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.0489 - accuracy: 0.5992\n",
      "Epoch 20/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.0331 - accuracy: 0.5823\n",
      "Epoch 21/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.0245 - accuracy: 0.5949\n",
      "Epoch 22/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 1.0383 - accuracy: 0.6076\n",
      "Epoch 23/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0189 - accuracy: 0.5907\n",
      "Epoch 24/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0013 - accuracy: 0.6076\n",
      "Epoch 25/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0051 - accuracy: 0.6160\n",
      "Epoch 26/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.9907 - accuracy: 0.6076\n",
      "Epoch 27/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9841 - accuracy: 0.6118\n",
      "Epoch 28/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9844 - accuracy: 0.6034\n",
      "Epoch 29/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9841 - accuracy: 0.6203\n",
      "Epoch 30/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9740 - accuracy: 0.6203\n",
      "Epoch 31/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.9737 - accuracy: 0.6034\n",
      "Epoch 32/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 0.9608 - accuracy: 0.6076\n",
      "Epoch 33/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.9628 - accuracy: 0.6160\n",
      "Epoch 34/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 1.0053 - accuracy: 0.5865\n",
      "Epoch 35/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9575 - accuracy: 0.6118\n",
      "Epoch 36/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9365 - accuracy: 0.6203\n",
      "Epoch 37/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.9504 - accuracy: 0.6118\n",
      "Epoch 38/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9560 - accuracy: 0.6160\n",
      "Epoch 39/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9310 - accuracy: 0.6203\n",
      "Epoch 40/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9282 - accuracy: 0.6287\n",
      "Epoch 41/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9352 - accuracy: 0.6160\n",
      "Epoch 42/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9215 - accuracy: 0.6245\n",
      "Epoch 43/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9148 - accuracy: 0.6329\n",
      "Epoch 44/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9157 - accuracy: 0.6245\n",
      "Epoch 45/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9146 - accuracy: 0.6245\n",
      "Epoch 46/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.9127 - accuracy: 0.6203\n",
      "Epoch 47/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9178 - accuracy: 0.6160\n",
      "Epoch 48/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.9148 - accuracy: 0.6076\n",
      "Epoch 49/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9202 - accuracy: 0.6245\n",
      "Epoch 50/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.9037 - accuracy: 0.6287\n",
      "Epoch 51/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8945 - accuracy: 0.6203\n",
      "Epoch 52/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9016 - accuracy: 0.6160\n",
      "Epoch 53/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8904 - accuracy: 0.6329\n",
      "Epoch 54/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8855 - accuracy: 0.6371\n",
      "Epoch 55/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8830 - accuracy: 0.6329\n",
      "Epoch 56/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 0.8808 - accuracy: 0.6329\n",
      "Epoch 57/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8787 - accuracy: 0.6371\n",
      "Epoch 58/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8756 - accuracy: 0.6498\n",
      "Epoch 59/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8814 - accuracy: 0.6371\n",
      "Epoch 60/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8799 - accuracy: 0.6371\n",
      "Epoch 61/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8921 - accuracy: 0.6245\n",
      "Epoch 62/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9263 - accuracy: 0.6076\n",
      "Epoch 63/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.9401 - accuracy: 0.6160\n",
      "Epoch 64/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8724 - accuracy: 0.6371\n",
      "Epoch 65/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.9008 - accuracy: 0.6118\n",
      "Epoch 66/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8941 - accuracy: 0.6414\n",
      "Epoch 67/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.8730 - accuracy: 0.6371\n",
      "Epoch 68/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8679 - accuracy: 0.6414\n",
      "Epoch 69/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8781 - accuracy: 0.6371\n",
      "Epoch 70/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8790 - accuracy: 0.6287\n",
      "Epoch 71/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8624 - accuracy: 0.6371\n",
      "Epoch 72/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8683 - accuracy: 0.6456\n",
      "Epoch 73/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8666 - accuracy: 0.6498\n",
      "Epoch 74/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8629 - accuracy: 0.6329\n",
      "Epoch 75/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8591 - accuracy: 0.6456\n",
      "Epoch 76/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8608 - accuracy: 0.6371\n",
      "Epoch 77/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8497 - accuracy: 0.6456\n",
      "Epoch 78/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8772 - accuracy: 0.6498\n",
      "Epoch 79/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8857 - accuracy: 0.6371\n",
      "Epoch 80/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8533 - accuracy: 0.6371\n",
      "Epoch 81/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8604 - accuracy: 0.6456\n",
      "Epoch 82/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8664 - accuracy: 0.6245\n",
      "Epoch 83/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8582 - accuracy: 0.6414\n",
      "Epoch 84/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8596 - accuracy: 0.6414\n",
      "Epoch 85/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8373 - accuracy: 0.6498\n",
      "Epoch 86/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.8706 - accuracy: 0.6245\n",
      "Epoch 87/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.8610 - accuracy: 0.6203\n",
      "Epoch 88/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8489 - accuracy: 0.6456\n",
      "Epoch 89/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8448 - accuracy: 0.6371\n",
      "Epoch 90/100\n",
      "237/237 [==============================] - 0s 76us/sample - loss: 0.8478 - accuracy: 0.6456\n",
      "Epoch 91/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8513 - accuracy: 0.6371\n",
      "Epoch 92/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8395 - accuracy: 0.6371\n",
      "Epoch 93/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.8443 - accuracy: 0.6329\n",
      "Epoch 94/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8348 - accuracy: 0.6329\n",
      "Epoch 95/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.8283 - accuracy: 0.6540\n",
      "Epoch 96/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8293 - accuracy: 0.6624\n",
      "Epoch 97/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8411 - accuracy: 0.6329\n",
      "Epoch 98/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8407 - accuracy: 0.6582\n",
      "Epoch 99/100\n",
      "237/237 [==============================] - 0s 72us/sample - loss: 0.8307 - accuracy: 0.6498\n",
      "Epoch 100/100\n",
      "237/237 [==============================] - 0s 68us/sample - loss: 0.8350 - accuracy: 0.6414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c18227f710>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the training data\n",
    "model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Improving Results - A Binary Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Interpretation: Although the model is fairly accurate, we still have a fairly large error. This could be because it is very difficult to distinguish between the different severity levels of heart disease (classes 1 - 4). Let's simplify the problem by converting the classification problem to a binary classification problem - heart disease or no heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#convert into binary classification problem - heart disease or no heart disease\n",
    "Y_train_binary = y_train.copy()\n",
    "Y_test_binary = y_test.copy()\n",
    "\n",
    "Y_train_binary[Y_train_binary > 0] = 1\n",
    "Y_test_binary[Y_test_binary > 0] = 1\n",
    "\n",
    "print (Y_train_binary[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 8)                 112       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define a new keras model for binary classification, for binary classification a sigmoid activation is more beneficial(push the value to 1 or 0).\n",
    "def create_binary_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "binary_model = create_binary_model()\n",
    "\n",
    "print(binary_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 237 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C1820AE0D0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C1820AE0D0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "237/237 [==============================] - 1s 2ms/sample - loss: 0.7082 - accuracy: 0.5527\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.6966 - accuracy: 0.5401\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.6858 - accuracy: 0.5612\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 0s 101us/sample - loss: 0.6757 - accuracy: 0.5612\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.6633 - accuracy: 0.5612\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.6550 - accuracy: 0.6076\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.6412 - accuracy: 0.6371\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.5969 - accuracy: 0.7004\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.6083 - accuracy: 0.6414\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 0s 105us/sample - loss: 0.5630 - accuracy: 0.7004\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 0s 101us/sample - loss: 0.5295 - accuracy: 0.7468\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.5275 - accuracy: 0.7004\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.5203 - accuracy: 0.7384\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.5046 - accuracy: 0.7637\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 0s 101us/sample - loss: 0.5071 - accuracy: 0.7173\n",
      "Epoch 16/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.4929 - accuracy: 0.7637\n",
      "Epoch 17/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4742 - accuracy: 0.7806\n",
      "Epoch 18/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4924 - accuracy: 0.7553\n",
      "Epoch 19/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4680 - accuracy: 0.7511\n",
      "Epoch 20/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4501 - accuracy: 0.7932\n",
      "Epoch 21/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4501 - accuracy: 0.7764\n",
      "Epoch 22/100\n",
      "237/237 [==============================] - 0s 90us/sample - loss: 0.4884 - accuracy: 0.7511\n",
      "Epoch 23/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4576 - accuracy: 0.7722\n",
      "Epoch 24/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4485 - accuracy: 0.7848\n",
      "Epoch 25/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4503 - accuracy: 0.7932\n",
      "Epoch 26/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4353 - accuracy: 0.7806\n",
      "Epoch 27/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4631 - accuracy: 0.7637\n",
      "Epoch 28/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4376 - accuracy: 0.7975\n",
      "Epoch 29/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4271 - accuracy: 0.8017\n",
      "Epoch 30/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4520 - accuracy: 0.7806\n",
      "Epoch 31/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4133 - accuracy: 0.8143\n",
      "Epoch 32/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.4268 - accuracy: 0.7890\n",
      "Epoch 33/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4438 - accuracy: 0.7848\n",
      "Epoch 34/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4266 - accuracy: 0.7975\n",
      "Epoch 35/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.4322 - accuracy: 0.8228\n",
      "Epoch 36/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.4142 - accuracy: 0.8143\n",
      "Epoch 37/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.4135 - accuracy: 0.7975\n",
      "Epoch 38/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4009 - accuracy: 0.8186\n",
      "Epoch 39/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3968 - accuracy: 0.8270\n",
      "Epoch 40/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4501 - accuracy: 0.7806\n",
      "Epoch 41/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.4179 - accuracy: 0.8059\n",
      "Epoch 42/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3976 - accuracy: 0.8143\n",
      "Epoch 43/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.3991 - accuracy: 0.8101\n",
      "Epoch 44/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3941 - accuracy: 0.8270\n",
      "Epoch 45/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3895 - accuracy: 0.8186\n",
      "Epoch 46/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4086 - accuracy: 0.8143\n",
      "Epoch 47/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3842 - accuracy: 0.8270\n",
      "Epoch 48/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.4077 - accuracy: 0.8101\n",
      "Epoch 49/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4162 - accuracy: 0.7932\n",
      "Epoch 50/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4213 - accuracy: 0.8017\n",
      "Epoch 51/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3828 - accuracy: 0.8270\n",
      "Epoch 52/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3785 - accuracy: 0.8270\n",
      "Epoch 53/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3904 - accuracy: 0.8228\n",
      "Epoch 54/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3846 - accuracy: 0.8354\n",
      "Epoch 55/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.4065 - accuracy: 0.8312\n",
      "Epoch 56/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3909 - accuracy: 0.8439\n",
      "Epoch 57/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3831 - accuracy: 0.8354\n",
      "Epoch 58/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.3808 - accuracy: 0.8439\n",
      "Epoch 59/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3985 - accuracy: 0.8312\n",
      "Epoch 60/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3920 - accuracy: 0.8397\n",
      "Epoch 61/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3972 - accuracy: 0.8312\n",
      "Epoch 62/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3822 - accuracy: 0.8439\n",
      "Epoch 63/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3785 - accuracy: 0.8481\n",
      "Epoch 64/100\n",
      "237/237 [==============================] - 0s 93us/sample - loss: 0.3812 - accuracy: 0.8312\n",
      "Epoch 65/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3839 - accuracy: 0.8397\n",
      "Epoch 66/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3710 - accuracy: 0.8397\n",
      "Epoch 67/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3894 - accuracy: 0.8312\n",
      "Epoch 68/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3711 - accuracy: 0.8228\n",
      "Epoch 69/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3797 - accuracy: 0.8312\n",
      "Epoch 70/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3767 - accuracy: 0.8397\n",
      "Epoch 71/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3880 - accuracy: 0.8397\n",
      "Epoch 72/100\n",
      "237/237 [==============================] - 0s 97us/sample - loss: 0.3823 - accuracy: 0.8481\n",
      "Epoch 73/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3876 - accuracy: 0.8439\n",
      "Epoch 74/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3827 - accuracy: 0.8354\n",
      "Epoch 75/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3873 - accuracy: 0.8228\n",
      "Epoch 76/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3724 - accuracy: 0.8565\n",
      "Epoch 77/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3672 - accuracy: 0.8523\n",
      "Epoch 78/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3715 - accuracy: 0.8650\n",
      "Epoch 79/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3670 - accuracy: 0.8481\n",
      "Epoch 80/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3714 - accuracy: 0.8354\n",
      "Epoch 81/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3799 - accuracy: 0.8312\n",
      "Epoch 82/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3698 - accuracy: 0.8397\n",
      "Epoch 83/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3631 - accuracy: 0.8523\n",
      "Epoch 84/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3698 - accuracy: 0.8523\n",
      "Epoch 85/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3831 - accuracy: 0.8608\n",
      "Epoch 86/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3788 - accuracy: 0.8692\n",
      "Epoch 87/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 0.3739 - accuracy: 0.8439\n",
      "Epoch 88/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3757 - accuracy: 0.8565\n",
      "Epoch 89/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3729 - accuracy: 0.8439\n",
      "Epoch 90/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3661 - accuracy: 0.8439\n",
      "Epoch 91/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3736 - accuracy: 0.8523\n",
      "Epoch 92/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3798 - accuracy: 0.8523\n",
      "Epoch 93/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 0.3837 - accuracy: 0.8481\n",
      "Epoch 94/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3786 - accuracy: 0.8439\n",
      "Epoch 95/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 0.3749 - accuracy: 0.8354\n",
      "Epoch 96/100\n",
      "237/237 [==============================] - 0s 89us/sample - loss: 0.3702 - accuracy: 0.8354\n",
      "Epoch 97/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3624 - accuracy: 0.8439\n",
      "Epoch 98/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3729 - accuracy: 0.8439\n",
      "Epoch 99/100\n",
      "237/237 [==============================] - 0s 80us/sample - loss: 0.3781 - accuracy: 0.8565\n",
      "Epoch 100/100\n",
      "237/237 [==============================] - 0s 84us/sample - loss: 0.3810 - accuracy: 0.8439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c184a25860>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the binary model on the training data\n",
    "binary_model.fit(X_train, Y_train_binary, epochs=100, batch_size=10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Results and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy results we have been seeing are for the training data, but what about the testing dataset? If our model's cannot generalize to data that wasn't used to train them, they won't provide any utility.\n",
    "\n",
    "Let's test the performance of both our categorical model and binary model. To do this, we will make predictions on the training dataset and calculate performance metrics using Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Categorical Model\n",
      "0.5166666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.83        27\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.14      0.29      0.19         7\n",
      "           3       0.33      0.40      0.36        10\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.52        60\n",
      "   macro avg       0.25      0.32      0.28        60\n",
      "weighted avg       0.41      0.52      0.46        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hwahwa\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# generate classification report using predictions for categorical model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "categorical_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "print('Results for Categorical Model')\n",
    "print(accuracy_score(y_test, categorical_pred))\n",
    "print(classification_report(y_test, categorical_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Binary Model\n",
      "0.8666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        27\n",
      "           1       0.88      0.88      0.88        33\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.87      0.87      0.87        60\n",
      "weighted avg       0.87      0.87      0.87        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate classification report using predictions for binary model \n",
    "binary_pred = np.round(binary_model.predict(X_test)).astype(int)\n",
    "\n",
    "print('Results for Binary Model')\n",
    "print(accuracy_score(Y_test_binary, binary_pred))\n",
    "print(classification_report(Y_test_binary, binary_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Interpretation: The accuracy, precision and recall score for the binary model is much better than the 5 categorical model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
